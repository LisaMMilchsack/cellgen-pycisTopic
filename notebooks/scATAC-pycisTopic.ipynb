{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Notebook version: 29 Jan 2021 \n",
    "(added small updates ~15 Feb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 # just to check whether the kernel finished loading..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eval \"$(ssh-agent -s)\"\n",
    "ssh-add ~/.ssh/id_rsa_VSC\n",
    "ssh -T git@github.com\n",
    "\n",
    "cd /staging/leuven/stg_00002/lcb/saibar/Projects/packages/\n",
    "git clone git@github.com:aertslab/pycisTopic.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"top\"></a>\n",
    "# Tutorial: scATAC-seq data analysis with pycisTopic\n",
    "\n",
    "* *Index:*\n",
    "    - [1. Getting pseudobulk profiles from scRNA-seq annotations](#pseudobulk)\n",
    "    - [2. Inferring consensus peaks](#consensus_peaks)\n",
    "    - [3. QC](#qc)\n",
    "        - [3a. Sample-level statistics](#stats_sample)\n",
    "        - [3b. Barcode level statistics](#stats_bc)\n",
    "    - [4. Creating a cisTopic object](#create_object)\n",
    "    - [5. Adding metadata to a cisTopic object](#add_metadata)\n",
    "    - [6. Running scrublet in a cisTopic object](#scrublet)\n",
    "    - [7. Run models](#run_models)\n",
    "    - [8. Model selection](#model_selection)\n",
    "    - [9. Clustering and visualization](#clustering)\n",
    "    - [10. Topic binarization](#topic_bin)\n",
    "    - [11. Differentially Accessible Regions (DARs)](#dars)\n",
    "    - [12. Exporting to loom](#loom_export)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Use your own copy from github to avoid issues)\n",
    "pycisTopicPath = '/staging/leuven/stg_00002/lcb/cbravo/Multiomics_pipeline/pycisTopic/'\n",
    "#pycisTopicPath = '/staging/leuven/stg_00002/lcb/saibar/Projects/SCENIC_V2/packages/pycisTopic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.chdir(pycisTopicPath)\n",
    "from pycisTopic.cistopic_class import *\n",
    "from pycisTopic.qc import *\n",
    "from pycisTopic.lda_models import *\n",
    "from pycisTopic.clust_vis import *\n",
    "from pycisTopic.topic_filtering import *\n",
    "from pycisTopic.diff_features import *\n",
    "from pycisTopic.pseudobulk_peak_calling import *\n",
    "from pycisTopic.iterative_peak_calling import *\n",
    "from pycisTopic.topic_binarization import *\n",
    "from pycisTopic.export_to_loom import *\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collected all imports to check dependencies (also kept in corresponding fields):\n",
    "import pyranges as pr\n",
    "import requests\n",
    "import pybiomart as pbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths and data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output path for this analysis:\n",
    "outDir = '/staging/leuven/stg_00002/lcb/saibar/Projects/SCENIC_V2/10x_multiome_brain/' + 'output/atac/pycistopic/'\n",
    "\n",
    "# Data location:\n",
    "datasetRootPath = '/staging/leuven/stg_00002/lcb/cbravo/Multiomics_pipeline/analysis/10x_multiome_brain/'\n",
    "path_to_annotated_rna_loom = datasetRootPath + 'output/rna/vsn/add_clusters_scrublet_as_annot/10x_multiome_brain_SCENIC_SCope_output_wAnnot_noDBL.loom'\n",
    "path_to_annotated_rna_loom_withDoublets = datasetRootPath + 'output/rna/vsn/add_clusters_scrublet_as_annot/10x_multiome_brain_SCENIC_SCope_output-wAnnot-wDBL.loom'\n",
    "fragments_dict = {'10x_multiome_brain': datasetRootPath + '/data/cell-arc/1.0.0/outs/human_brain_3k_atac_fragments.tsv.gz'}\n",
    "\n",
    "# Settings/databases, etc...:\n",
    "path_to_blacklist = pycisTopicPath + 'blacklist/hg38-blacklist.v2.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)\n",
    "os.listdir(outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"pseudobulk\"></a>\n",
    "### 1. Getting pseudobulk profiles from scRNA-seq annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the barcode metadata from the scRNA-seq analysis. In this case, I will use the annotated loom file, but loading it from a tsv file or similar is also possible (as long as you end up with a similar pd.DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get metadata from loom (to add to SCopeLoomPy)\n",
    "def get_metadata(loom):\n",
    "    annot_metadata = loom.get_meta_data()['annotations']\n",
    "    annot_mt_column_names = [annot_metadata[x]['name'] for x in range(len(annot_metadata))]\n",
    "    annot_mt = pd.concat([pd.DataFrame(loom.col_attrs[annot_mt_column_names[x]]) for x in range(len(annot_mt_column_names))], axis=1)\n",
    "    annot_mt.columns = [annot_mt_column_names[x] for x in range(len(annot_mt_column_names))]\n",
    "    annot_mt.index = loom.get_cell_ids().tolist()\n",
    "    return annot_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata from loom file\n",
    "loom = SCopeLoom.read_loom(path_to_annotated_rna_loom)\n",
    "cell_data = get_metadata(loom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, if you are starting from a pandas data frame, the index of your pandas should correspond to BARCODE (e.g. ATGTCTGATAGA-1, additional tags are possible using -; e.g. ATGTCTGATAGA-1-sample_1) and it must contain a 'sample_id' column indicating the sample label fo origin. Let's see how it looks like here. The sample_id for all cells in this tutorial is '10x_multiome_brain'. Alternatively, you can also add a column named 'barcode' to the metadata with the corresponding cell barcodes (in this case the name of the cells will not be used to infer the barcode id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to produce the bigwig files, we also need to know the overall size of the chromosomes. We can easily download this information from the UCSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chromosome sizes (for hg38 here)\n",
    "import pyranges as pr\n",
    "import requests\n",
    "chromSizes_url='http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes'\n",
    "chromsizes=pd.read_csv(chromSizes_url, sep='\\t', header=None)\n",
    "chromsizes.columns=['Chromosome', 'End']\n",
    "chromsizes['Start']=[0]*chromsizes.shape[0]\n",
    "chromsizes=chromsizes.loc[:,['Chromosome', 'Start', 'End']]\n",
    "# Exceptionally in this case, to agree with CellRangerARC annotations\n",
    "chromsizes['Chromosome'] = [chromsizes['Chromosome'][x].replace('v', '.') for x in range(len(chromsizes['Chromosome']))]\n",
    "chromsizes['Chromosome'] = [chromsizes['Chromosome'][x].split('_')[1] if len(chromsizes['Chromosome'][x].split('_')) > 1 else chromsizes['Chromosome'][x] for x in range(len(chromsizes['Chromosome']))]\n",
    "chromsizes=pr.PyRanges(chromsizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the ingredients we need  to generate the pseudobulk files. With this function we will generate fragments files per group and the corresponding bigwigs. The mandatory input to this function are:\n",
    "* The annotation dataframe (`input_data`)\n",
    "* The variable used to group the cells (in this case, 'cell type')\n",
    "* The chromosome sizes\n",
    "* The paths to where the bed and bigiwg files will be written\n",
    "* A dictionary indicating the fragments file corresponsing to each sample. **The sample ids used as keys in this dictionary must match with the sample ids in the annotation data frame!**\n",
    "\n",
    "The output will be two dictionaries containing the paths to the bed and bigwig files, respectively, to each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_paths, bed_paths = export_pseudobulk(input_data = cell_data,\n",
    "                 variable = 'cell_type',\n",
    "                 chromsizes = chromsizes,\n",
    "                 bed_path = outDir + 'consensus_peak_calling/pseudobulk_bed_files/',\n",
    "                 bigwig_path =  outDir + 'consensus_peak_calling/pseudobulk_bw_files/',\n",
    "                 path_to_fragments = fragments_dict,\n",
    "                 n_cpu = 5,\n",
    "                 normalize_bigwig = True,\n",
    "                 remove_duplicates = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(outDir+\"consensus_peak_calling/pseudobulk_bed_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(outDir+'consensus_peak_calling/pseudobulk_bw_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the paths dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outDir + 'consensus_peak_calling/pseudobulk_bed_files/bed_paths.pkl', 'wb') as f:\n",
    "  pickle.dump(bed_paths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outDir + 'consensus_peak_calling/pseudobulk_bed_files/bw_paths.pkl', 'wb') as f:\n",
    "  pickle.dump(bw_paths, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used with cisTopic objects (as `input_data`), instead of the annotation data frame and the paths to fragments dictionary. You can find an example later."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# to reload:\n",
    "with open(outDir+'consensus_peak_calling/pseudobulk_bed_files/bed_paths.pkl', 'rb') as f:\n",
    "    bed_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"consensus_peaks\"></a>\n",
    "### 2. Inferring consensus peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will use MACS2 to call peaks in each group (in this case, cell type). The default parameters are those recommended for ATAC-seq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set correct path to run MACS2\n",
    "os.putenv('MACS2_HOME','/data/leuven/software/biomed/haswell_centos7/2018a/software/MACS2/2.1.2.1-foss-2018a-Python-2.7.16/')\n",
    "os.putenv('PATH','/data/leuven/software/biomed/haswell_centos7/2018a/software/MACS2/2.1.2.1-foss-2018a-Python-2.7.16/bin:' + os.environ['PATH'])\n",
    "macs_path =' /data/leuven/software/biomed/haswell_centos7/2018a/software/MACS2/2.1.2.1-foss-2018a-Python-2.7.16/bin/macs2'\n",
    "macs_outdir = outDir + 'consensus_peak_calling/MACS/'\n",
    "\n",
    "# Run peak calling\n",
    "narrow_peaks_dict = peak_calling(macs_path, bed_paths, macs_outdir, n_cpu=5,\n",
    "                    genome_size='hs', input_format='BEDPE', shift=73, ext_size=146, keep_dup = 'all', q_value = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the narrow peaks dictionary (with a PyRanges with the narrow peaks for each cell type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(macs_outdir + 'narrow_peaks_dict.pkl', 'wb') as f:\n",
    "  pickle.dump(narrow_peaks_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is time to derive the consensus peaks. To do so, we use the TGCA iterative peak filtering approach. First, each summit is extended a `peak_half_width` in each direction and then we iteratively filter out less significant peaks that overlap with a more significant one. During this procedure peaks will be merged and depending on the number of peaks included into them, different processes will happen:\n",
    "* **1 peak**:  The original peak region will be kept\n",
    "* **2 peaks**:  The original peak region with the highest score will be kept\n",
    "* **3 or more peaks**:  The orignal peak region with the most significant score will be taken, and all the original peak regions in this merged peak region that overlap with the significant peak region will be removed. The process is repeated with the next most significant peak (if it was not removed already) until all peaks are processed.\n",
    "\n",
    "This proccess will happen twice, first in each pseudobulk peaks; and after peak score normalization, to process all peaks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chromosome sizes (for hg38 here). We need them to ensure that extending the summits we don't fall out of the chromosome.\n",
    "import pyranges as pr\n",
    "import requests\n",
    "chromSizes_url='http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes'\n",
    "chromsizes=pd.read_csv(chromSizes_url, sep='\\t', header=None)\n",
    "chromsizes.columns=['Chromosome', 'End']\n",
    "chromsizes['Start']=[0]*chromsizes.shape[0]\n",
    "chromsizes=chromsizes.loc[:,['Chromosome', 'Start', 'End']]\n",
    "# Exceptionally in this case, to agree with CellRangerARC annotations\n",
    "chromsizes['Chromosome'] = [chromsizes['Chromosome'][x].replace('v', '.') for x in range(len(chromsizes['Chromosome']))]\n",
    "chromsizes['Chromosome'] = [chromsizes['Chromosome'][x].split('_')[1] if len(chromsizes['Chromosome'][x].split('_')) > 1 else chromsizes['Chromosome'][x] for x in range(len(chromsizes['Chromosome']))]\n",
    "chromsizes=pr.PyRanges(chromsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other param\n",
    "peak_half_width=250\n",
    "# Get consensus peaks\n",
    "consensus_peaks=get_consensus_peaks(narrow_peaks_dict, peak_half_width, chromsizes=chromsizes, path_to_blacklist=path_to_blacklist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to bed\n",
    "consensus_peaks.to_bed(path=outDir + 'consensus_peak_calling/consensus_regions.bed', keep=True, compression='infer', chain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"qc\"></a>\n",
    "### 3. QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to perform QC in the scATAC-seq samples (in this case, only one run). There are several measurements and visualizations performed in this step:\n",
    "\n",
    "* Barcode rank plot\n",
    "* Duplication rate\n",
    "* Insertion size\n",
    "* TSS enrichment\n",
    "* Fraction of Reads In Peaks (FRIP)\n",
    "\n",
    "To calculate the TSS enrichment we need to provide TSS annotations. You can easily download them via pybiomart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TSS annotations\n",
    "import pybiomart as pbm\n",
    "# For mouse\n",
    "#dataset = pbm.Dataset(name='mmusculus_gene_ensembl',  host='http://www.ensembl.org')\n",
    "# For human\n",
    "dataset = pbm.Dataset(name='hsapiens_gene_ensembl',  host='http://www.ensembl.org')\n",
    "# For fly\n",
    "#dataset = pbm.Dataset(name='dmelanogaster_gene_ensembl',  host='http://www.ensembl.org')\n",
    "annot = dataset.query(attributes=['chromosome_name', 'transcription_start_site', 'strand', 'external_gene_name', 'transcript_biotype'])\n",
    "filter = annot['Chromosome/scaffold name'].str.contains('CHR|GL|JH|MT')\n",
    "annot = annot[~filter]\n",
    "annot['Chromosome/scaffold name'] = annot['Chromosome/scaffold name'].str.replace(r'(\\b\\S)', r'chr\\1')\n",
    "annot.columns=['Chromosome', 'Start', 'Strand', 'Gene', 'Transcript_type']\n",
    "annot = annot[annot.Transcript_type == 'protein_coding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run all (or several of) the metrics, you can use the `compute_qc_stats()` function. As input you need to provide a dictionary containing the fragments files per sample and another dictionary the corresponding regions to use to estimate the FRIP. For more details in the QC stats, see the *QC* advanced tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_regions_qc = {'10x_multiome_brain': outDir + 'consensus_peak_calling/consensus_regions.bed'}\n",
    "\n",
    "metadata_bc, profile_data_dict = compute_qc_stats(fragments_dict = fragments_dict,\n",
    "                tss_annotation = annot,\n",
    "                stats=['barcode_rank_plot', 'duplicate_rate', 'insert_size_distribution', 'profile_tss', 'frip'],\n",
    "                label_list = None,\n",
    "                path_to_regions = path_to_regions_qc,\n",
    "                n_cpu = 5,\n",
    "                valid_bc = None,\n",
    "                n_frag = 100,\n",
    "                n_bc = None,\n",
    "                tss_flank_window = 1000,\n",
    "                tss_window = 50,\n",
    "                tss_minimum_signal_window = 100,\n",
    "                tss_rolling_window = 10,\n",
    "                remove_duplicates = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outDir + 'quality_control'):\n",
    "    os.makedirs(outDir + 'quality_control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outDir + 'quality_control/metadata_bc.pkl', 'wb') as f:\n",
    "  pickle.dump(metadata_bc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outDir + 'quality_control/profile_data_dict.pkl', 'wb') as f:\n",
    "  pickle.dump(profile_data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"stats_sample\"></a>\n",
    "#### 3a. Sample-level statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the QC metrics have been computed you can visualize the results at the sample-level and the barcode-level. Sample-level statistics can be used to assess the overall quality of the sample, while barcode level statistics can be use to differentiate good quality cells versus the rest. The sample-level graphs include:\n",
    "\n",
    "* **Barcode rank plot**: The barcode rank plot shows the distribution of non-duplicate reads and which barcodes were inferred to be associated with cells. A steep drop-off ('knee') is indicative of good separation between the cell-associated barcodes and the barcodes associated with empty partitions.\n",
    "\n",
    "* **Insertion size**: ATAC-seq requires a proper pair of Tn5 transposase cutting events at the ends of DNA. In the nucleosome-free open chromatin regions, many molecules of Tn5 can kick in and chop the DNA into small pieces; around nucleosome-occupied regions, and Tn5 can only access the linker regions. Therefore, in a good ATAC-seq library, you should expect to see a sharp peak at the <100 bp region (open chromatin), and a peak at ~200bp region (mono-nucleosome), and other larger peaks (multi-nucleosomes). A clear nucleosome pattern indicates a good quality of the experiment.\n",
    "\n",
    "* **Sample TSS enrichment**: The TSS enrichment calculation is a signal to noise calculation. The reads around a reference set of TSSs are collected to form an aggregate distribution of reads centered on the TSSs and extending to 1000 bp in either direction (for a total of 2000bp). This distribution is then normalized by taking the average read depth in the 100 bps at each of the end flanks of the distribution (for a total of 200bp of averaged data) and calculating a fold change at each position over that average read depth. This means that the flanks should start at 1, and if there is high read signal at transcription start sites (highly open regions of the genome) there should be an increase in signal up to a peak in the middle.\n",
    "\n",
    "* **FRIP distribution**: Fraction of all mapped reads that fall into the called peak regions, i.e. usable reads in significantly enriched peaks divided by all usable reads. In general, FRiP scores correlate positively with the number of regions. A low FRIP indicates that many reads form part of the background, and so that the data is noisy.\n",
    "\n",
    "* **Duplication rate**: A fragment is considered “usable” if it uniquely maps to the genome and remains after removing PCR duplicates (defined as two fragments that map to the same genomic position and have the same unique molecular identifier). The duplication rate serves to estimate the amount of usable reads per barcode. High duplication rates may indicate over-sequencing or lack of fragments after transposition and encapsulation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load sample metrics\n",
    "import pickle\n",
    "infile = open(outDir + 'quality_control/profile_data_dict.pkl', 'rb')\n",
    "profile_data_dict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_metrics(profile_data_dict,\n",
    "           insert_size_distriubtion_xlim=[0,600],\n",
    "           ncol=5,\n",
    "           save=outDir + 'quality_control/sample_metrics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"stats_bc\"></a>\n",
    "#### 3b. Barcode level statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barcode-level statistics can be used to select high quality cells. Typical measurements that can be used are:\n",
    "\n",
    "* **Total number of (unique) fragments**\n",
    "* **TSS enrichment**: The score at position in the TSS enrichmen score for for each barcode (at position 0, the TSS). Noisy cells will have a low TSS enrichment.\n",
    "* **FRIP**: The fraction of reads in peaks for each barcode. Noisy cells have low FRIP values. However, this filter should be used with nuance, as it depends on the quality of the original peaks. For example, if there is a rare population in the sample, its specific peaks may be missed by peak calling algorithms, causing a decrease in their FRIP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load barcode metrics\n",
    "# import pickle\n",
    "# infile = open(outDir + 'quality_control/metadata_bc.pkl', 'rb')\n",
    "# metadata_bc = pickle.load(infile)\n",
    "# infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include in plot_barcode_metrics?\n",
    "if not os.path.exists(outDir + 'quality_control/barcode_metrics'):\n",
    "    os.makedirs(outDir + 'quality_control/barcode_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return figure to plot together with other metrics, and cells passing filters. Figure will be saved as pdf.\n",
    "FRIP_NR_FRAG_fig, FRIP_NR_FRAG_filter = plot_barcode_metrics(metadata_bc['10x_multiome_brain'],\n",
    "                                       var_x='Log_unique_nr_frag',\n",
    "                                       var_y='FRIP',\n",
    "                                       min_x=3.5,\n",
    "                                       max_x=None,\n",
    "                                       min_y=20,\n",
    "                                       max_y=None,\n",
    "                                       return_cells=True,\n",
    "                                       return_fig=True,\n",
    "                                       plot=False,\n",
    "                                       save=outDir + 'quality_control/barcode_metrics/FRIP-VS-NRFRAG.pdf')\n",
    "# Return figure to plot together with other metrics, and cells passing filters\n",
    "TSS_NR_FRAG_fig, TSS_NR_FRAG_filter=plot_barcode_metrics(metadata_bc['10x_multiome_brain'],\n",
    "                                      var_x='Log_unique_nr_frag',\n",
    "                                      var_y='TSS_enrichment',\n",
    "                                      min_x=3.5,\n",
    "                                      max_x=None,\n",
    "                                      min_y=5,\n",
    "                                      max_y=None,\n",
    "                                      return_cells=True,\n",
    "                                      return_fig=True,\n",
    "                                      plot=False,\n",
    "                                      save=outDir + 'quality_control/barcode_metrics/TSS-VS-NRFRAG.pdf')\n",
    "# Return figure to plot together with other metrics, but not returning cells (no filter applied for the duplication rate  per barcode)\n",
    "DR_NR_FRAG_fig=plot_barcode_metrics(metadata_bc['10x_multiome_brain'],\n",
    "                                      var_x='Log_unique_nr_frag',\n",
    "                                      var_y='Dupl_rate',\n",
    "                                      min_x=3.5,\n",
    "                                      max_x=None,\n",
    "                                      min_y=5,\n",
    "                                      max_y=None,\n",
    "                                      return_cells=False,\n",
    "                                      return_fig=True,\n",
    "                                      plot=False)\n",
    "\n",
    "# Plot barcode stats in one figure\n",
    "fig=plt.figure(figsize=(40,10))\n",
    "plt.subplot(1, 3, 1)\n",
    "img = fig2img(FRIP_NR_FRAG_fig) #To convert figures to png to plot together, see .utils.py. This converts the figure to png.\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "img = fig2img(TSS_NR_FRAG_fig)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "img = fig2img(DR_NR_FRAG_fig)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have information about doublet barcodes from the scRNA-seq analysis. We will remove these barcodes from our selected barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cells marked as doublets in the scRNA-seq analysis\n",
    "loom = SCopeLoom.read_loom(path_to_annotated_rna_loom_withDoublets)\n",
    "cell_data = get_metadata(loom)\n",
    "# Here I apply a corection to get the correct barcode name\n",
    "SCRUBLET_doublets = [re.sub(\"-1.0.0\", \"-1\", x) for x in cell_data[cell_data['scrublet__predicted_doublets'] == 1].index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_passing_filters = {'10x_multiome_brain':[]}\n",
    "bc_passing_filters['10x_multiome_brain'] = list((set(FRIP_NR_FRAG_filter) & set(TSS_NR_FRAG_filter)) ^ set(SCRUBLET_doublets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 3,154 selected barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bc_passing_filters['10x_multiome_brain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, a total of 2,419 overlaps with high quality scRNA-seq barcodes. We will keep the additional barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata from high-quality loom file\n",
    "loom = SCopeLoom.read_loom(path_to_annotated_rna_loom)\n",
    "cell_data = get_metadata(loom)\n",
    "scRNA_bc=[re.sub(\"-10x_multiome_brain\", \"\", x) for x in cell_data.index.tolist()]\n",
    "#len(scRNA_bc) 2,607 cells\n",
    "len(list(set(bc_passing_filters['10x_multiome_brain']) & set(scRNA_bc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outDir + 'quality_control/bc_passing_filters.pkl', 'wb') as f:\n",
    "  pickle.dump(bc_passing_filters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"create_object\"></a>\n",
    "### 4. Creating a cisTopic object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will generate a **count matrix** with the fragments in each region for each barcode. \n",
    "If you would like to start from a precomputed fragments count matrix it is also possible (see advanced tutorial on *Initializing cisTopic objects*). \n",
    "For multiple samples, you can add additional entries in `fragment_dict`. \n",
    "As regions, we will use the consensus peaks derived from the scRNA-seq annotations. \n",
    "\n",
    "This cisTopic object will contain:\n",
    "\n",
    "* Path/s to fragment file/s (if generated from fragments files)\n",
    "* Fragment count matrix and binary accessibility matrix\n",
    "* Cell and region metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_regions_ct = outDir + 'consensus_peak_calling/consensus_regions.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "infile = open(outDir + 'quality_control/metadata_bc.pkl', 'rb')\n",
    "metadata_bc = pickle.load(infile)\n",
    "infile.close()\n",
    "# Valid barcodes\n",
    "infile = open(outDir + 'quality_control/bc_passing_filters.pkl', 'rb')\n",
    "bc_passing_filters = pickle.load(infile)\n",
    "infile.close()\n",
    "#Create objects\n",
    "cistopic_obj_list=[create_cistopic_object_from_fragments(path_to_fragments=fragments_dict[key],\n",
    "                                               path_to_regions=path_to_regions_ct,\n",
    "                                               path_to_blacklist=path_to_blacklist,\n",
    "                                               metrics=metadata_bc[key],\n",
    "                                               valid_bc=bc_passing_filters[key],\n",
    "                                               n_cpu=5,\n",
    "                                               project=key) for key in fragments_dict.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we only have one sample, so only one cisTopic object has been generated. If you would have multiple samples, you would need to run the `merge()` function in your cisTopic object list. You can find more information in the advanced tutorial on *Initializing cisTopic objects*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cistopic_obj = cistopic_obj_list[0]\n",
    "print(cistopic_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open(outDir + '10x_multiome_brain_cisTopicObject.pkl', 'wb') as f:\n",
    "  pickle.dump(cistopic_obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"add_metadata\"></a>\n",
    "### 5. Adding metadata to a cisTopic object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add additional metadata (for regions or cells) to a cisTopic object. For example, let's add the scRNA-seq data annotations. For those barcodes that did not pass the scRNA-seq values will be filled with `Nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cisTopic object\n",
    "infile = open(outDir + '10x_multiome_brain_cisTopicObject.pkl', 'rb')\n",
    "cistopic_obj = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get metadata from loom (to add to SCopeLoomPy)\n",
    "def get_metadata(loom):\n",
    "    annot_metadata = loom.get_meta_data()['annotations']\n",
    "    annot_mt_column_names = [annot_metadata[x]['name'] for x in range(len(annot_metadata))]\n",
    "    annot_mt = pd.concat([pd.DataFrame(loom.col_attrs[annot_mt_column_names[x]]) for x in range(len(annot_mt_column_names))], axis=1)\n",
    "    annot_mt.columns = [annot_mt_column_names[x] for x in range(len(annot_mt_column_names))]\n",
    "    annot_mt.index = loom.get_cell_ids().tolist()\n",
    "    return annot_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata from loom file\n",
    "loom = SCopeLoom.read_loom(path_to_annotated_rna_loom)\n",
    "cell_data = get_metadata(loom).drop('sample_id', axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexes in the pandas data frame to add can be **cell barcodes** (if the cisTopic object has been created from a fragments file only) or an **exact match with the cell names** in the cisTopic object (`cistopic_obj.cell_names`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cistopic_obj.add_cell_data(cell_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cistopic_obj.cell_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"scrublet\"></a>\n",
    "### 6. Running scrublet in a cisTopic object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can run also scrublet on the fragment count matrix to infer doublets from the scATAC-seq. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrublet as scr\n",
    "scrub = scr.Scrublet(cistopic_obj.fragment_matrix.T, expected_doublet_rate=0.1)\n",
    "doublet_scores, predicted_doublets = scrub.scrub_doublets()\n",
    "scrub.plot_histogram();\n",
    "scrub.call_doublets(threshold=0.35)\n",
    "scrub.plot_histogram();\n",
    "scrublet = pd.DataFrame([scrub.doublet_scores_obs_, scrub.predicted_doublets_], columns=cistopic_obj.cell_names, index=['Doublet_scores_fragments', 'Predicted_doublets_fragments']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cistopic_obj.add_cell_data(scrublet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "141 cells are called as doublets. We will keep them to see how the look in the analysis, but you can already remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cistopic_obj.cell_data.Predicted_doublets_fragments == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with doublets\n",
    "with open(outDir + '10x_multiome_brain_cisTopicObject.pkl', 'wb') as f:\n",
    "  pickle.dump(cistopic_obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset all cells marked as singlets from the cisTopic object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove doublets \n",
    "singlets = cistopic_obj.cell_data[cistopic_obj.cell_data.Predicted_doublets_fragments == False].index.tolist()\n",
    "# Subset cisTopic object\n",
    "cistopic_obj_noDBL = cistopic_obj.subset(singlets, copy=True)\n",
    "print(cistopic_obj_noDBL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save without doublets\n",
    "with open(outDir + '10x_multiome_brain_cisTopicObject_noDBL.pkl', 'wb') as f:\n",
    "  pickle.dump(cistopic_obj_noDBL, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run the LDA models. There are two types of LDA models (with Collapsed Gibbs Sampling) you can run:\n",
    "\n",
    "* **Serial LDA**: The parallelization is done between models rather than within each model. Recommended for small-medium sized data sets in which several models with different number os topics are being tested. You can run these models with `runCGSModels()`.\n",
    "* **Parallel LDA with MALLET**: The parallelization is done within each model. Recommended for large data sets where a few models with different number of topics are being tested. If working in a cluster, we recommed to submit a job per model so they can run simultaneously. You can run it with `runCGSModelsMallet()`.\n",
    "\n",
    "In this tutorial we will use `runCGSModels`. For more details in how to run models, see the advanced tutorial on *Running LDA models*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=run_cgs_models(cistopic_obj,\n",
    "                    n_topics=[2,10,20,30,40,50],\n",
    "                    n_cpu=6,\n",
    "                    n_iter=100,\n",
    "                    random_state=555,\n",
    "                    alpha=50,\n",
    "                    alpha_by_topic=True,\n",
    "                    eta=0.1,\n",
    "                    eta_by_topic=False,\n",
    "                    save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add models with other numbers of topics (e.g. run at a later time...): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_add=run_cgs_models(cistopic_obj,\n",
    "                    n_topics=[5,15,25,35,45],\n",
    "                    n_cpu=6,\n",
    "                    n_iter=100,\n",
    "                    random_state=555,\n",
    "                    alpha=50,\n",
    "                    alpha_by_topic=True,\n",
    "                    eta=0.1,\n",
    "                    eta_by_topic=False,\n",
    "                    save_path=None)\n",
    "models = models + models_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "if not os.path.exists(outDir + 'models/'):\n",
    "    os.makedirs(outDir + 'models/')\n",
    "with open(outDir + 'models/10x_multiome_brain_models_100_iter.pkl', 'wb') as f:\n",
    "  pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a cluster and want to run several models, we recommend to submit this step as a job. You can use the `runModels_lda.py` script."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash -l\n",
    "## Job will last 6 hours.\n",
    "#PBS -l walltime=6:00:00\n",
    "## Job needs 1 nodes and 24 cores per node.\n",
    "#PBS -l nodes=1:ppn=28\n",
    "## Job request memory\n",
    "#PBS -lmem=180gb # or 180gb\n",
    "## Specify project credits name to use credits for running the job.\n",
    "#PBS -A lp_symbiosys\n",
    "## Batch job name.\n",
    "#PBS -N 10x_multiomics_brain\n",
    "## Email options.\n",
    "#PBS -m abe\n",
    "#PBS -M carmen.bravogonzalezblas@kuleuven.vib.be\n",
    "\n",
    "module load Python/3.7.4-foss-2018a\n",
    "cd /staging/leuven/stg_00002/lcb/cbravo/Multiomics_pipeline/pycisTopic\n",
    "python job_template/runModels_lda.py \\\n",
    "        -i /staging/leuven/stg_00002/lcb/cbravo/Multiomics_pipeline/analysis/10x_multiome_brain/output/atac/pycistopic/10x_multiome_brain_cisTopicObject.pkl \\\n",
    "        -o /staging/leuven/stg_00002/lcb/cbravo/Multiomics_pipeline/analysis/10x_multiome_brain/output/atac/pycistopic/models/10x_multiome_brain_models_300_iter.pkl \\\n",
    "        -nt 2,5,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,35,40,45,50 \\\n",
    "        -c 27 \\\n",
    "        -it 300 \\\n",
    "        -a 50 \\\n",
    "        -abt True \\\n",
    "        -e 0.1 \\\n",
    "        -ebt False \\\n",
    "        -sp None \\\n",
    "        -s 555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model_selection\"></a>\n",
    "### 8. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods that can be used for model selection:\n",
    "\n",
    "* **Minmo_2011**: Uses the average model coherence as calculated by Mimno et al (2011). In order to reduce the impact of the number of topics, we calculate the average coherence based on the top selected average values. The better the model, the higher coherence.\n",
    "* **Log-likelihood**: Uses the log-likelihood in the last iteration as calculated by Griffiths and Steyvers (2004). The better the model, the higher the log-likelihood.\n",
    "* **Arun_2010**: Uses a density-based metric as in Arun et al (2010) using the topic-region distribution, the cell-topic distribution and the cell coverage. The better the model, the lower the metric.\n",
    "* **Cao_Juan_2009**: Uses a divergence-based metric as in Cao Juan et al (2009) using the topic-region distribution. The better the model, the lower the metric.\n",
    "\n",
    "For scATAC-seq data models, the most helpful methods are Minmo (topic coherence) and the log-likelihood in the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load models\n",
    "# infile = open(outDir + 'models/10x_multiome_brain_models_100_iter.pkl', 'rb')\n",
    "# models = pickle.load(infile)\n",
    "# infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order:\n",
    "models=[models[i] for i in np.argsort([m.n_topic for m in models])]\n",
    "[m.n_topic for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=evaluate_models(models,\n",
    "                     select_model=25, \n",
    "                     return_model=True, \n",
    "                     metrics=['Arun_2010','Cao_Juan_2009', 'Minmo_2011', 'loglikelihood'],\n",
    "                     plot_metrics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cisTopic object\n",
    "infile = open(outDir + '10x_multiome_brain_cisTopicObject.pkl', 'rb')\n",
    "cistopic_obj = pickle.load(infile)\n",
    "infile.close()\n",
    "# Add model to cisTopicObject\n",
    "cistopic_obj.add_LDA_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open(outDir + '10x_multiome_brain_cisTopicObject.pkl', 'wb') as f:\n",
    "  pickle.dump(cistopic_obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering\"></a>\n",
    "### 9. Clustering and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cluster the cells (or regions) using the leiden algorithm, and perfor dimensionality reductiion with UMAP and TSNE. In these examples we will focus on the cells only. For these steps, the cell-topic contriibutions of the model will be used."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load cisTopic object\n",
    "infile = open(outDir + '10x_multiome_brain_cisTopicObject.pkl', 'rb')\n",
    "cistopic_obj = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_clusters(cistopic_obj,\n",
    "                 target  = 'cell',\n",
    "                 k = 10,\n",
    "                 res = 0.6,\n",
    "                 prefix = 'pycisTopic_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_umap(cistopic_obj, target  = 'cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tsne(cistopic_obj, target  = 'cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering assignments are added to `cistopic_obj.cell_data` and the projections to `cistopic_obj.projections['cell']`. If you would like to add additional dimensionality reductions, you can just add them as an entry to the projections dictionary (under 'cell' in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cistopic_obj.cell_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize metadata (categorical or continuous) on the UMAP/tSNE spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include in plot_metadata?\n",
    "if not os.path.exists(outDir + 'visualization'):\n",
    "    os.makedirs(outDir + 'visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metadata(cistopic_obj,\n",
    "                 reduction_name='UMAP',\n",
    "                 variables=['cell_type', 'pycisTopic_leiden_10_0.6'], # Labels from RNA and new clusters\n",
    "                 target='cell', num_columns=2,\n",
    "                 text_size=24,\n",
    "                 dot_size=15,\n",
    "                 figsize=(25,10),\n",
    "                 save=outDir + 'visualization/dimensionality_reduction_label.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check now where our predicted doublets fall. Many of these predicted doublets actually correspond to cells with no high quality RNA profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metadata(cistopic_obj,\n",
    "                 reduction_name='UMAP',\n",
    "                 variables=['Log_unique_nr_frag', 'TSS_enrichment', 'Doublet_scores_fragments', 'Predicted_doublets_fragments'], \n",
    "                 target='cell', num_columns=4,\n",
    "                 text_size=15,\n",
    "                 save=outDir + 'visualization/dimensionality_reduction_number.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the topic-contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic(cistopic_obj,\n",
    "            reduction_name = 'UMAP',\n",
    "            target = 'cell',\n",
    "            num_columns=4,\n",
    "            save=outDir + 'visualization/dimensionality_reduction_topic_contr.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can also draw a heatmap with the topic contributions (and annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_topic_heatmap(cistopic_obj,\n",
    "                     variables = ['cell_type'],\n",
    "                     scale = True,\n",
    "                     legend_loc_x = 1.05,\n",
    "                     legend_loc_y = -1.2,\n",
    "                     legend_dist_y = -1,\n",
    "                     figsize=(10,10),\n",
    "                     save = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"topic_bin\"></a>\n",
    "### 10. Topic binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dars\"></a>\n",
    "### 11. Differentially Accessible Regions (DARs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"loom_export\"></a>\n",
    "### 12. Exporting to loom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to top]](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycisTopic - r23i27n22",
   "language": "python",
   "name": "rik_ssh_genius_r23i27n22_pycistopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
